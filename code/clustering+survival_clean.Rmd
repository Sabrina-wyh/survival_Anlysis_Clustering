---
title: "survival analysis"
author: "Yihan Wang"
date: "2025-01-04"
output: word_document
---

```{r}
rm(list = ls())
```

```{r}
library(survival)
library(randomForestSRC)
library(broom)
library(ggplot2)
library(dplyr)
library(caret)
library(pROC)
library(mRMRe)
library(kml)
library(rgl)
library(longitudinalData)
library(doParallel)
library(foreach)
library(dtw)
library(cluster)  
library(factoextra)
library(parallel)
library(survival)
library(timeROC)
library(dplyr)
```


```{r}
summarize_classification_progression <- function(df) {
  # Group data by ID and extract unique progression patterns
  progression_dict <- df %>%
    group_by(ID) %>%
    summarize(
      Progression = list(Classification[c(TRUE, diff(as.numeric(factor(Classification))) != 0)])
    ) %>%
    pull(Progression)
  
  unique_ids <- unique(df$ID)
  summary_map <- list()
  
  for (i in seq_along(unique_ids)) {
    id <- unique_ids[i]
    progression <- unlist(progression_dict[i])
    progression_key <- paste(progression, collapse = " -> ")
    if (!is.null(summary_map[[progression_key]])) {
      summary_map[[progression_key]] <- c(summary_map[[progression_key]], id)
    } else {
      summary_map[[progression_key]] <- list(id)
    }
  }
  for (pattern in names(summary_map)) {
    ids <- unlist(summary_map[[pattern]])
    cat(sprintf("Progression Pattern: %s -> RIDs: %s\n", pattern, paste(ids, collapse = ", ")))
    cat(sprintf("with %d participants\n\n", length(ids)))
  }
}

```

```{r}
perform_mrmr_feature_selection <- function(data_frame, columns_to_exclude, feature_count) {
  feature_selection_df <- data.frame(data_frame[, !colnames(data_frame) %in% columns_to_exclude])
  data_mrmr <- mRMR.data(data = feature_selection_df)
  column_names <- featureNames(data_mrmr)
  target_index <- which(column_names == "event")
  
  if (length(target_index) == 0) {
    stop("Target column 'event' not found in the dataset")
  }
  
  feature_selection <- mRMR.classic(data = data_mrmr, target_indices = target_index, feature_count = feature_count)
  selected_indices <- feature_selection@filters[[1]]
  selected_features <- column_names[selected_indices]
  return(selected_features)
}
```

```{r}
multivariate_dtw <- function(series1, series2) {
    sum(sapply(seq_along(series1), function(f) {
      dtw(series1[[f]], series2[[f]], keep = FALSE)$distance
    }))
  }

compute_dtw_distance_matrix <- function(data_frame, selected_features) {
  cluster_df <- data_frame[, c("ID", selected_features)]
  cluster_df[, selected_features] <- scale(cluster_df[, selected_features])
  
  feature_data_clustering <- lapply(selected_features, function(feature) {
    split(cluster_df[[feature]], cluster_df$ID)
  })
  
  ts_data_clustering <- lapply(unique(cluster_df$ID), function(id) {
    lapply(seq_along(selected_features), function(f) feature_data_clustering[[f]][[as.character(id)]])
  })
  names(ts_data_clustering) <- unique(cluster_df$ID)
  
  # parallel 
  cl <- makeCluster(detectCores() - 1)  # Use all cores except one
  registerDoParallel(cl)
  
  multivariate_dtw <- function(series1, series2) {
    sum(sapply(seq_along(series1), function(f) {
      dtw(series1[[f]], series2[[f]], keep = FALSE)$distance
    }))
  }
  
  n_subjects <- length(unique(cluster_df$ID))
  distance_matrix_dtw <- matrix(0, n_subjects, n_subjects)
  distance_matrix_dtw <- foreach(i = 1:n_subjects, .combine = rbind, .packages = "dtw") %:% 
    foreach(j = i:n_subjects, .combine = c) %dopar% {
      if (i < j) {
        dist <- multivariate_dtw(ts_data_clustering[[i]], ts_data_clustering[[j]])
        dist
      } else {
        0
      }
    }
  distance_matrix_dtw <- distance_matrix_dtw + t(distance_matrix_dtw)
  stopCluster(cl)
  return(distance_matrix_dtw)
}
```




```{r}
NACC_MCI_df <- read.csv("longitudinal_NACC_HC_Jan9.csv")
print(length(unique(NACC_MCI_df$ID)))
excluded_columns <- c("Classification", "APOE", "start", "stop", "ID", "APOE4", "age")
feature_count <- 10
selected_features <- perform_mrmr_feature_selection(NACC_MCI_df, excluded_columns, feature_count)
distance_matrix <- compute_dtw_distance_matrix(NACC_MCI_df, selected_features)
```

```{r}
fviz_nbclust(distance_matrix, kmeans, method = "wss") +
  labs(title = "Elbow Method for Optimal Clusters")

fviz_nbclust(distance_matrix, kmeans, method = "silhouette") +
  labs(title = "Silhouette Method for Optimal Clusters")
```

```{r}
optimal_k <- 2
set.seed(42)
kmeans_res <- kmeans(distance_matrix, centers = optimal_k, nstart = 10)
id_cluster_mapping <- data.frame(ID = unique(NACC_MCI_df$ID), Cluster = kmeans_res$cluster)

for (k in 1:optimal_k) {
  cat(sprintf("\n--- Cluster %d ---\n", k))
  cluster_ids <- id_cluster_mapping$ID[id_cluster_mapping$Cluster == k]
  cluster_data <- NACC_MCI_df %>%
    filter(ID %in% cluster_ids)
  
  total_participants <- length(unique(cluster_data$ID))
  participants_with_event <- sum(cluster_data$event == 1, na.rm = TRUE)
  
  # Calculate the percentage
  percentage_with_event <- (participants_with_event / total_participants) * 100
  
  cat(sprintf("Total participants: %d\n", total_participants))
  cat(sprintf("Participants with Event == 1: %d\n", participants_with_event))
  cat(sprintf("Percentage of participants with Event == 1: %.2f%%\n", percentage_with_event))

}

```


```{r}
common_time <- seq(min(NACC_MCI_df$start), max(NACC_MCI_df$start), length.out = 100)
cluster_colors <- rainbow(optimal_k)

# Initialize a list to store centroid trajectories
centroid_trajectories <- list()
for (feature in selected_features) {
  centroid_trajectories[[feature]] <- lapply(1:optimal_k, function(k) {
    cluster_ids <- id_cluster_mapping$ID[id_cluster_mapping$Cluster == k]
    cluster_data <- NACC_MCI_df %>%
      filter(ID %in% cluster_ids) %>%
      select(ID, start, all_of(feature))
    feature_matrix <- do.call(rbind, lapply(split(cluster_data, cluster_data$ID), function(df) {
      approx(x = df$start, y = df[[feature]], xout = common_time, rule = 2)$y
    }))
    colMeans(feature_matrix, na.rm = TRUE)
  })
}

plot_data <- do.call(rbind, lapply(selected_features, function(feature) {
  do.call(rbind, lapply(1:optimal_k, function(k) {
    data.frame(
      Time = common_time,
      Value = centroid_trajectories[[feature]][[k]],
      Cluster = as.factor(k),
      Feature = feature
    )
  }))
}))

# Plot the centroid trajectories
ggplot(plot_data, aes(x = Time, y = Value, color = Cluster)) +
  geom_line(size = 1) +
  facet_wrap(~ Feature, scales = "free_y") +
  labs(title = "Centroid Trajectories by Feature",
       x = "Time",
       y = "Value",
       color = "Cluster") +
  theme_minimal()
```

doing the external validation
```{r}
assign_new_participants <- function(new_data, centroid_trajectories, common_time, selected_features) {
  sapply(unique(new_data$ID), function(id) {
    participant_data <- new_data %>% filter(ID == id)
    cluster_distances <- sapply(1:length(centroid_trajectories[[1]]), function(cluster_index) {
      sum(sapply(selected_features, function(feature) {
        participant_trajectory <- interpolate_trajectory(participant_data, common_time, feature)
        centroid_trajectory <- centroid_trajectories[[feature]][[cluster_index]]
        dtw(participant_trajectory, centroid_trajectory, keep = FALSE)$distance
      }))
    })
    which.min(cluster_distances)
  })
}

interpolate_trajectory <- function(participant_data, common_time, feature) {
  approx(
    x = participant_data$start,
    y = participant_data[[feature]],
    xout = common_time,
    rule = 2
  )$y
}
```

```{r}
assign_clusters_to_new_participants <- function(df, feature_list, centroid_trajectories, common_time) {
  feature_data <- lapply(feature_list, function(feature) {
    split(df[[feature]], df$ID)
  })
  
  ts_data <- lapply(unique(df$ID), function(id) {
    lapply(seq_along(feature_list), function(f) feature_data[[f]][[as.character(id)]])
  })
  
  new_cluster_assignments <- assign_new_participants(
    new_data = df,
    centroid_trajectories = centroid_trajectories,
    common_time = common_time,
    selected_features = feature_list
  )
  
  id_cluster_mapping <- data.frame(
    ID = unique(df$ID),
    Assigned_Cluster = new_cluster_assignments
  )
  return(id_cluster_mapping)
}
```



```{r}
AIBL_MCI_df <- read.csv("longitudinal_AIBL_HC_Jan9.csv")
print(length(unique(AIBL_MCI_df$ID)))
id_cluster_mapping_AIBL <- assign_clusters_to_new_participants(
  df = AIBL_MCI_df,
  feature_list = selected_features,
  centroid_trajectories = centroid_trajectories,
  common_time = seq(min(AIBL_MCI_df$start), max(AIBL_MCI_df$start), length.out = 100)
)
```



```{r}
NACC_MCI_df <- read.csv("longitudinal_NACC_HC_Jan9.csv")
colnames(NACC_MCI_df)
NACC_MCI_df <- NACC_MCI_df %>% select(-Classification, -APOE, -APOE4)
# NACC_MCI_df <- NACC_MCI_df %>% select("start", "stop", "event", "ID", "APOE4", "age")
AIBL_MCI_df <- read.csv("longitudinal_AIBL_HC_Jan9.csv")
AIBL_MCI_df <- AIBL_MCI_df %>% select(-Classification, -APOE, -APOE4)
# AIBL_MCI_df <- AIBL_MCI_df %>% select("start", "stop", "event", "ID", "APOE4", "age")
```


```{r}
predictors <- setdiff(names(NACC_MCI_df), c("start", "stop", "event", "ID"))
formula <- as.formula(paste("Surv(start, stop, event) ~", paste(predictors, collapse = " + ")))
cox_model <- coxph(formula, data = NACC_MCI_df, id = ID, ties = "efron")
summary(cox_model)

risk_scores <- predict(cox_model, newdata = AIBL_MCI_df, type = "risk")
  
# Compute c-index using survival package
c_index <- concordance(cox_model, newdata = AIBL_MCI_df)$concordance
cat(sprintf("C-index = %.3f\n", c_index))

# Compute AUC
times <- unique(AIBL_MCI_df$stop)
times <- as.numeric(times)
times <- times[!is.na(times) & times > 0]  # Remove NA and non-positive times

auc_result <- timeROC(
  T = AIBL_MCI_df$stop,
  delta = AIBL_MCI_df$event,
  marker = risk_scores,
  cause = 1,
  times = times
)
print(auc_result)
```

############# first visit only #################
```{r}
NACC_MCI_df <- read.csv("longitudinal_NACC_full_one_record_Jan9.csv")
colnames(NACC_MCI_df)
NACC_MCI_df <- NACC_MCI_df %>% select(-Classification, -APOE)
# NACC_MCI_df <- NACC_MCI_df %>% select("time", "event", "ID", "APOE4", "age")
AIBL_MCI_df <- read.csv("longitudinal_AIBL_full_one_record_Jan9.csv")
AIBL_MCI_df <- AIBL_MCI_df %>% select(-Classification, -APOE)
# AIBL_MCI_df <- AIBL_MCI_df %>% select("time", "event", "ID", "APOE4", "age")
```

```{r}
predictors <- setdiff(names(NACC_MCI_df), c("time", "event", "ID"))
formula <- as.formula(paste("Surv(time, event) ~", paste(predictors, collapse = " + ")))
cox_model <- coxph(formula, data = NACC_MCI_df)
summary(cox_model)

risk_scores <- predict(cox_model, newdata = AIBL_MCI_df, type = "risk")
  
# Compute c-index using survival package
c_index <- concordance(cox_model, newdata = AIBL_MCI_df)$concordance
cat(sprintf("C-index = %.3f\n", c_index))

# Compute AUC
times <- unique(AIBL_MCI_df$time)
times <- as.numeric(times)
times <- times[!is.na(times) & times > 0]  # Remove NA and non-positive times

auc_result <- timeROC(
  T = AIBL_MCI_df$time,
  delta = AIBL_MCI_df$event,
  marker = risk_scores,
  cause = 1,
  times = times
)
print(auc_result)
```







```{r}
results <- list()
for (k in 1:optimal_k) {
  cat(sprintf("\n--- Cluster %d ---\n", k))
  cluster_ids <- id_cluster_mapping$ID[id_cluster_mapping$Cluster == k]
  cluster_train_data <- NACC_MCI_df %>% filter(ID %in% cluster_ids)
  if (nrow(cluster_train_data) < 2) {
    cat(sprintf("Cluster %d has insufficient training data. Skipping.\n", k))
    next
  }
  # cluster_train_data <- cluster_train_data %>% select(-Classification, -APOE)
  
  # Check for highly correlated variables and drop
  predictors <- setdiff(names(cluster_train_data), c("start", "stop", "event", "ID"))
  corr_matrix <- cor(cluster_train_data %>% select(all_of(predictors)), use = "pairwise.complete.obs")
  high_corr_pairs <- which(abs(corr_matrix) > 0.9, arr.ind = TRUE)
  high_corr_pairs <- high_corr_pairs[high_corr_pairs[, 1] < high_corr_pairs[, 2], , drop = FALSE]
  
  if (!is.null(nrow(high_corr_pairs)) && nrow(high_corr_pairs) > 0) {
    vars_to_remove <- unique(rownames(high_corr_pairs))
    cat("Highly correlated variables detected. Dropping the following variables:\n")
    print(vars_to_remove)
    predictors <- setdiff(predictors, vars_to_remove)
    cluster_train_data <- cluster_train_data %>% select(-all_of(vars_to_remove))
  }
  
  cluster_train_data <- cluster_train_data %>% mutate(across(all_of(predictors), ~ scale(.)[, 1]))
  formula <- as.formula(paste("Surv(start, stop, event) ~", paste(predictors, collapse = " + ")))
  initial_cox_model <- coxph(formula, data = cluster_train_data, id = ID, ties = "exact")
  # print(summary(initial_cox_model))
  
  # Select significant features (p < 0.05) or use the original predictors
  significant_features <- rownames(summary(initial_cox_model)$coefficients)[summary(initial_cox_model)$coefficients[, "Pr(>|z|)"] <= 0.05]
  
  if (length(significant_features) <= 2) {
    cat(sprintf("Cluster %d has no significant predictors. Using original predictors.\n", k))
    significant_features <- selected_features
  }
  
  formula_retrained <- as.formula(paste("Surv(start, stop, event) ~", paste(significant_features, collapse = " + ")))
  final_cox_model <- coxph(formula_retrained, data = cluster_train_data, id = ID, ties = "breslow")
  print(summary(final_cox_model))
  
  # Testing
  cluster_test_ids <- id_cluster_mapping_AIBL$ID[id_cluster_mapping_AIBL$Assigned_Cluster == k]
  cluster_test_data <- AIBL_MCI_df %>% filter(ID %in% cluster_test_ids)
  if (nrow(cluster_test_data) < 1) {
    cat(sprintf("Cluster %d has insufficient test data. Skipping.\n", k))
    next
  }
  cluster_test_data <- cluster_test_data %>% select(all_of(c("ID", "start", "stop", "event")), all_of(significant_features))
  cluster_test_data <- cluster_test_data %>% mutate(across(all_of(significant_features), ~ scale(.)[, 1]))
  
  # Evaluate the model using the test set
  test_surv_obj <- Surv(cluster_test_data$start, cluster_test_data$stop, cluster_test_data$event)
  risk_scores <- predict(final_cox_model, newdata = cluster_test_data, type = "risk")
  
  # Compute c-index using survival package
  c_index <- concordance(final_cox_model, newdata = cluster_test_data)$concordance
  cat(sprintf("Cluster %d: C-index = %.3f\n", k, c_index))
  
  # Compute AUC
  times <- unique(cluster_test_data$stop)
  times <- as.numeric(times)
  times <- times[!is.na(times) & times > 0]  # Remove NA and non-positive times
  
  auc_result <- timeROC(
    T = cluster_test_data$stop,
    delta = cluster_test_data$event,
    marker = risk_scores,
    cause = 1,
    times = times
  )
  print(auc_result)
}

```









